{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3",
   "metadata": {},
   "source": [
    "# üï∏Ô∏è Web Scraping with Python: Mining the Internet\n",
    "\n",
    "## Welcome to Web Scraping!\n",
    "\n",
    "## About the course and Author\n",
    "\n",
    "This comprehensive Python training program is designed to take you from a beginner to a proficient\n",
    "Python programmer. Through seven carefully structured modules, you‚Äôll learn fundamental programming\n",
    "concepts, advanced features, and practical applications of Python\n",
    "\n",
    "**About the Author:**\n",
    "\n",
    "* NDE HURICH DILAN\n",
    "* Full Stack Ai Web & Mobile Developer\n",
    "* Junior Researcher in NLP and AI \n",
    "* Contact: ndedilan504@gmail.com or via https://techwithdilan.tech\n",
    "* WhatsApp: +237 694 52 59 31\n",
    "\n",
    "Copyright: All rights reserved\n",
    "\n",
    "---\n",
    "Please note that this course provides only an introduction to python. For deeper insights, please refer to other resources such as:\n",
    "\n",
    "* [Official Python Tutorial](https://docs.python.org/3/tutorial/index.html)\n",
    "* [W3Schools Python Tutorial](https://www.w3schools.com/python/) - A beginner-friendly resource with interactive examples and explanations for learning Python step-by-step.\n",
    "* [Real Python](https://realpython.com/) - A comprehensive site with tutorials, guides, and examples for Python programming at all levels.\n",
    "* [GeeksforGeeks Python Programming](https://www.geeksforgeeks.org/python-programming-language/) - Beginner to advanced Python tutorials, with a focus on programming concepts and problem-solving.\n",
    "* [Python.org Official Tutorial](https://docs.python.org/3/tutorial/) - The official Python documentation, offering a structured and detailed introduction to Python for beginners.\n",
    "* [Codecademy Python Course](https://www.codecademy.com/learn/learn-python-3) - An interactive, hands-on course designed for beginners to learn Python fundamentals.\n",
    "* [Kaggle Learn Python](https://www.kaggle.com/learn/python) - Short, practical exercises tailored for beginners, especially those interested in data science.\n",
    "* [FreeCodeCamp Python Tutorials](https://www.freecodecamp.org/news/tag/python/) - A collection of beginner-friendly Python tutorials and projects, covering a wide range of topics.\n",
    "* [Sololearn Python Course](https://www.sololearn.com/Course/Python/) - A mobile-friendly platform with bite-sized Python lessons and challenges for beginners.\n",
    "\n",
    "Learn how to extract and analyze data from websites automatically - like\n",
    "a data ninja!\n",
    "\n",
    "### What We‚Äôll Learn üéØ\n",
    "\n",
    "-   What is web scraping and why it‚Äôs awesome\n",
    "-   Key libraries (Requests, BeautifulSoup)\n",
    "-   Scraping techniques and best practices\n",
    "-   Real-world scraping projects\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## 1. Web Scraping Basics: Your Digital Mining Tools ‚õèÔ∏è\n",
    "\n",
    "### What is Web Scraping? ü§î\n",
    "\n",
    "Web scraping is like having a super-fast assistant who can read websites\n",
    "and collect information for you automatically!\n",
    "\n",
    "``` python\n",
    "# First, let's import our key libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Simple example: Getting a webpage\n",
    "def fetch_webpage(url):\n",
    "    \"\"\"Fetch a webpage and return its content\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for errors\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Test with a simple webpage\n",
    "url = \"http://quotes.toscrape.com\"\n",
    "content = fetch_webpage(url)\n",
    "print(\"Successfully fetched the webpage!\" if content else \"Failed to fetch webpage\")\n",
    "```\n",
    "\n",
    "### Why Web Scraping is Awesome! üåü\n",
    "\n",
    "``` python\n",
    "\"\"\"\n",
    "Benefits of Web Scraping:\n",
    "1. Automate data collection\n",
    "2. Save hours of manual work\n",
    "3. Get real-time data\n",
    "4. Build powerful datasets\n",
    "5. Monitor prices and changes\n",
    "\"\"\"\n",
    "\n",
    "# Example: Scraping quotes\n",
    "def scrape_quotes(url):\n",
    "    \"\"\"Scrape quotes from quotes.toscrape.com\"\"\"\n",
    "    content = fetch_webpage(url)\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    quotes = []\n",
    "    for quote in soup.find_all('div', class_='quote'):\n",
    "        text = quote.find('span', class_='text').text\n",
    "        author = quote.find('small', class_='author').text\n",
    "        quotes.append({'quote': text, 'author': author})\n",
    "    \n",
    "    return quotes\n",
    "\n",
    "# Let's get some quotes!\n",
    "quotes = scrape_quotes(url)\n",
    "for i, quote in enumerate(quotes[:3], 1):\n",
    "    print(f\"\\nQuote {i}:\")\n",
    "    print(f\"üìù {quote['quote']}\")\n",
    "    print(f\"‚úçÔ∏è - {quote['author']}\")\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## 2. Essential Tools: Your Scraping Toolkit üß∞\n",
    "\n",
    "``` python\n",
    "# BeautifulSoup Basics\n",
    "def soup_tutorial():\n",
    "    html_example = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <h1>Welcome to Scraping</h1>\n",
    "            <div class=\"content\">\n",
    "                <p class=\"important\">This is important text</p>\n",
    "                <p>This is normal text</p>\n",
    "                <a href=\"https://example.com\">Click here</a>\n",
    "            </div>\n",
    "        </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html_example, 'html.parser')\n",
    "    \n",
    "    print(\"üîç Finding Elements:\")\n",
    "    print(f\"Title: {soup.h1.text}\")\n",
    "    print(f\"Important text: {soup.find('p', class_='important').text}\")\n",
    "    print(f\"Link href: {soup.a['href']}\")\n",
    "\n",
    "# Run the tutorial\n",
    "soup_tutorial()\n",
    "```\n",
    "\n",
    "### üéØ Practice: Build a Simple Scraper\n",
    "\n",
    "``` python\n",
    "def scrape_weather(city):\n",
    "    \"\"\"Simulate weather scraping (for educational purposes)\"\"\"\n",
    "    weather_data = {\n",
    "        'New York': {'temp': '72¬∞F', 'condition': 'Sunny'},\n",
    "        'London': {'temp': '18¬∞C', 'condition': 'Rainy'},\n",
    "        'Tokyo': {'temp': '25¬∞C', 'condition': 'Cloudy'}\n",
    "    }\n",
    "    \n",
    "    return weather_data.get(city, {'temp': 'N/A', 'condition': 'City not found'})\n",
    "\n",
    "# Test the weather scraper\n",
    "cities = ['New York', 'London', 'Tokyo']\n",
    "for city in cities:\n",
    "    weather = scrape_weather(city)\n",
    "    print(f\"\\nüåç Weather in {city}:\")\n",
    "    print(f\"üå°Ô∏è Temperature: {weather['temp']}\")\n",
    "    print(f\"‚òÅÔ∏è Condition: {weather['condition']}\")\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## 3. Best Practices: Being a Good Scraping Citizen ü§ù\n",
    "\n",
    "``` python\n",
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Educational Purpose Only)'\n",
    "        }\n",
    "    \n",
    "    def scrape_with_delay(self, urls, delay=2):\n",
    "        \"\"\"Scrape multiple URLs with a polite delay\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for url in urls:\n",
    "            # Be polite, wait between requests\n",
    "            time.sleep(delay)\n",
    "            \n",
    "            try:\n",
    "                response = self.session.get(url)\n",
    "                results.append(response.text)\n",
    "                print(f\"‚úÖ Successfully scraped: {url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to scrape {url}: {e}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Example usage\n",
    "scraper = WebScraper()\n",
    "test_urls = [\n",
    "    \"http://quotes.toscrape.com/page/1\",\n",
    "    \"http://quotes.toscrape.com/page/2\"\n",
    "]\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## 4. Real-World Project: Building a Book Scraper üìö\n",
    "\n",
    "``` python\n",
    "class BookScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"http://books.toscrape.com\"\n",
    "        self.scraper = WebScraper()\n",
    "    \n",
    "    def scrape_book_details(self, book_url):\n",
    "        \"\"\"Scrape details of a single book\"\"\"\n",
    "        content = self.scraper.session.get(book_url).text\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "        title = soup.find('h1').text\n",
    "        price = soup.find('p', class_='price_color').text\n",
    "        availability = soup.find('p', class_='availability').text.strip()\n",
    "        \n",
    "        return {\n",
    "            'title': title,\n",
    "            'price': price,\n",
    "            'availability': availability\n",
    "        }\n",
    "    \n",
    "    def scrape_category(self, category_url, limit=5):\n",
    "        \"\"\"Scrape books from a category\"\"\"\n",
    "        books = []\n",
    "        content = self.scraper.session.get(category_url).text\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "        for book in soup.find_all('article', class_='product_pod')[:limit]:\n",
    "            title = book.h3.a['title']\n",
    "            price = book.find('p', class_='price_color').text\n",
    "            books.append({'title': title, 'price': price})\n",
    "        \n",
    "        return books\n",
    "\n",
    "# Test the book scraper\n",
    "book_scraper = BookScraper()\n",
    "science_books = book_scraper.scrape_category(\n",
    "    \"http://books.toscrape.com/catalogue/category/books/science_22/index.html\"\n",
    ")\n",
    "\n",
    "print(\"\\nüìö Science Books Available:\")\n",
    "for book in science_books:\n",
    "    print(f\"\\nüìñ {book['title']}\")\n",
    "    print(f\"üí∞ {book['price']}\")\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## 5. Fun Project: Building a News Headline Aggregator üì∞\n",
    "\n",
    "``` python\n",
    "class HeadlineAggregator:\n",
    "    def __init__(self):\n",
    "        self.sources = {\n",
    "            'Tech': 'http://example.com/tech',  # Replace with real URLs\n",
    "            'Science': 'http://example.com/science',\n",
    "            'Sports': 'http://example.com/sports'\n",
    "        }\n",
    "    \n",
    "    def get_headlines(self, category):\n",
    "        \"\"\"Simulate getting headlines (for educational purposes)\"\"\"\n",
    "        sample_headlines = {\n",
    "            'Tech': [\n",
    "                \"New AI Breakthrough Announced\",\n",
    "                \"Latest Smartphone Released\",\n",
    "                \"Quantum Computing Milestone Reached\"\n",
    "            ],\n",
    "            'Science': [\n",
    "                \"Mars Mission Update\",\n",
    "                \"New Species Discovered\",\n",
    "                \"Climate Study Released\"\n",
    "            ],\n",
    "            'Sports': [\n",
    "                \"Championship Finals Today\",\n",
    "                \"New World Record Set\",\n",
    "                \"Star Player Signs Deal\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return sample_headlines.get(category, [\"Category not found\"])\n",
    "\n",
    "# Test the headline aggregator\n",
    "aggregator = HeadlineAggregator()\n",
    "categories = ['Tech', 'Science', 'Sports']\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"\\nüì∞ {category} Headlines:\")\n",
    "    for headline in aggregator.get_headlines(category):\n",
    "        print(f\"‚û§ {headline}\")\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You‚Äôve learned: - Web scraping basics - Using BeautifulSoup and\n",
    "Requests - Best practices and ethics - Real-world applications\n",
    "\n",
    "### üìö Practice Ideas\n",
    "\n",
    "1.  Build a price comparison tool\n",
    "2.  Create a news aggregator\n",
    "3.  Track product prices\n",
    "4.  Build a job listing scraper\n",
    "\n",
    "### üéØ Challenge Project: Multi-Site Data Aggregator\n",
    "\n",
    "Create a system that: - Scrapes data from multiple sources - Compares\n",
    "prices across websites - Tracks changes over time - Exports data to\n",
    "CSV/Excel\n",
    "\n",
    "Remember: Always check a website‚Äôs robots.txt and terms of service\n",
    "before scraping! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
